---
title: "3.6 Modeling - Decision Tree"
author: "Himanshu"
date: "April 15, 2019"
output: html_document
---

```{r}
library(randomForest)
library(dplyr)
library(caret)
library(lubridate)
library(stringr)
library(tidyr)
library(corrplot)
library(ggcorrplot)
library(leaps)
library(MASS)
library(ggplot2)
library(standardize)
library(rpart)
library(mlbench)
library(caret)
library(tidyverse)
library(rpart.plot)
library(RColorBrewer)
```

```{r}
train <- read.csv("TrainData.csv", header = TRUE)
test <- read.csv("TestData.csv", header = TRUE)
str(train)
str(test)

```
```{r}
vars_to_remove <- c("X.1", "X","pymnt_plan", "GeoName", "application_type")

train <- train[, ! names(train) %in% vars_to_remove, drop = F]
str(train)

test <- test[, ! names(test) %in% vars_to_remove, drop = F]
str(test)
```


```{r}
# We'll do stratified sampling to split our data into training and test sets
set.seed(1)

targetVar <- 'default'
inTrain <- createDataPartition(y = df1[,targetVar], list = FALSE, p = .8)
train <- df1[inTrain,]
test <- df1[-inTrain,]
stopifnot(nrow(train) + nrow(test) == nrow(df1))
str(df1)
```

```{r}
targetVar <- 'default'
xVars <- c("loan_amnt", "term", "int_rate", "installment", "grade",
           "home_ownership", "annual_inc", "verification_status", "purpose",
           "dti", "open_acc", "pub_rec", "total_acc", "initial_list_status", 
           "Compensation.of.employees", "Gross.operating.surplus", "Per.capita.real.GDP.by.state", 
           "Quantity.indexes.for.real.GDP.by.state", "Real.GDP.by.state", "Subsidies", 
           "Taxes.on.production.and.imports", "regions" )
```


```{r}
# Imbalanced data - under-sampling
# Under-sampling balances the dataset by reducing the size of the abundant class. This method is used when quantity of # data is sufficient. By keeping all samples in the rare class and randomly selecting an equal number of samples in   # the abundant class, a balanced new dataset can be retrieved for further modelling.

# train_down <- 
#   caret::downSample(x = train[, xVars], 
#                     y = as.factor(train$default), yname = "default")
# 
# base::prop.table(table(train_down$default))
# base::table(train_down$default)
```

```{r}
# Model Formula

createModelFormula <- function(targetVar, xVars, includeIntercept = TRUE){
  if(includeIntercept){
    modelForm <- as.formula(paste(targetVar, "~", paste(xVars, collapse = '+ ')))
  } else {
    modelForm <- as.formula(paste(targetVar, "~", paste(xVars, collapse = '+ '), -1))
  }
  return(modelForm)
}

modelForm <- createModelFormula(targetVar, xVars)

```

```{r}
require(tree)
tree.default = tree(default~loan_amnt+term+int_rate+installment+grade+home_ownership+annual_inc+verification_status+purpose+dti+open_acc+pub_rec+total_acc+initial_list_status+Compensation.of.employees+Gross.operating.surplus+Per.capita.real.GDP.by.state+ Quantity.indexes.for.real.GDP.by.state+Real.GDP.by.state+Subsidies+Taxes.on.production.and.imports+regions, data=train)
summary(tree.default)

plot(tree.default)
text(tree.default, pretty = 0)

```



```{r}
#Base model fro decision tree 

targetVar<- train$default
fit <- rpart(modelForm,
             data=train,
             method="class")

fit
summary(fit)

prp(fit)

## Predicting on test data
fit.pred = predict(fit, test, type="class")

## misclassification table
with(test, table(fit.pred, default))

## Confusion matrix 
test[,'fit.pred'] <- fit.pred
confusionMatrix(table(test$fit.pred, test$default))
```
```{r}
varImp(fit)

#   Problem : Plotting VarImp in barplot
#   varImp(fit)$importance %>%
#   as.data.frame() %>%
#   rownames_to_column() %>%
#   arrange(Overall) %>%
#   mutate(rowname = forcats::fct_inorder(rowname )) %>%
#   ggplot()+
#     geom_col(aes(x = rowname, y = Overall))+
#     coord_flip()+
#     theme_bw()

# most important variables are interest rate, Initial list status, Quantity.indexes.for.real.GDP.by.state

```

```{r}
#cross validation to optimizes Hyperparameter tuning on decision tree

train_ctrl <- trainControl(method = "repeatedcv"
                       , number = 10, repeats = 3
                       , classProbs = TRUE
                       , summaryFunction = twoClassSummary  
                       , search = "grid"
                       , verboseIter = FALSE
                       , allowParallel = TRUE
                       )
x = train[,xVars]


y = as.factor(train[, targetVar])
cleanNames <- function(x){
feature.names=names(x)
  for (f in feature.names) {
    if (class(x[[f]])=="factor") {
      levels <- unique(c(x[[f]]))
      x[[f]] <- factor(x[[f]],
                       labels=make.names(levels))
    }
  }
return(x)}

xOld <- x
x <- cleanNames(x)
y = make.names(y)

Cross_fit<- train(x = x
               , y = y
               , method = "rpart",
               # This is telling caret to test 20 options of the
               # hyperparameters (tuning variables) for this
               # model. Supposedly caret should know for rpart
               # what variables to tune. In practice, bugs are
               # aplenty!
               tuneLength=20,
               metric = "ROC",
               trControl = train_ctrl)



Cross_fit

```

```{r}
# Let's extract the model that caret developed at plot it

ggplot(Cross_fit)
fit2 <- Cross_fit$finalModel
prp(fit2)

test <- cleanNames(test)

fit2.pred <- predict(Cross_fit, test, type = "raw")

str(test)
str(fit2.pred)

levels(test$default) <- c("FALSE.","TRUE.")
#test$default <- as.factor(test$default)
str(test$default)
ActualNew <-test$default

confusionMatrix(reference = ActualNew, data = fit2.pred)
varImp(Cross_fit)
```

```{r}
test1 <- read.csv("TestData.csv", header = TRUE)
test1[,'fit.pred'] <- fit2.pred
test1$fit.pred <- as.factor(test1$fit.pred)
levels(test1$fit.pred) <- c(FALSE, TRUE)

test1$fit.pred
write.csv(test, file = "DecisionTreeTest.csv" )
```


















