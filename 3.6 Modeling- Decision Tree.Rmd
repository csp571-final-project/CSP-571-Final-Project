---
title: "3.6 Modeling - Decision Tree"
author: "Himanshu"
date: "April 15, 2019"
output: html_document
---

```{r}
library(randomForest)
library(dplyr)
library(caret)
library(lubridate)
library(stringr)
library(tidyr)
library(corrplot)
library(ggcorrplot)
library(leaps)
library(MASS)
library(ggplot2)
library(standardize)
library(rpart)
library(mlbench)
library(caret)
library(tidyverse)
library(rpart.plot)
library(RColorBrewer)
```

```{r}
train <- read.csv("TrainData.csv", header = TRUE)
test <- read.csv("TestData.csv", header = TRUE)
str(train)
str(test)

```
```{r}
vars_to_remove <- c("X.1", "X","pymnt_plan", "GeoName", "application_type")

train <- train[, ! names(train) %in% vars_to_remove, drop = F]
str(train)

test <- test[, ! names(test) %in% vars_to_remove, drop = F]
str(test)
```

```{r}
targetVar <- 'default'
xVars <- c("loan_amnt", "term", "int_rate", "installment", "grade",
           "home_ownership", "annual_inc", "verification_status", "purpose",
           "dti", "open_acc", "pub_rec", "total_acc", "initial_list_status", 
           "Compensation.of.employees", "Gross.operating.surplus", "Per.capita.real.GDP.by.state", 
           "Quantity.indexes.for.real.GDP.by.state", "Real.GDP.by.state", "Subsidies", 
           "Taxes.on.production.and.imports", "regions" )
```


```{r}
# Imbalanced data - under-sampling
# Under-sampling balances the dataset by reducing the size of the abundant class. This method is used when quantity of # data is sufficient. By keeping all samples in the rare class and randomly selecting an equal number of samples in   # the abundant class, a balanced new dataset can be retrieved for further modelling.

# train_down <- 
#   caret::downSample(x = train[, xVars], 
#                     y = as.factor(train$default), yname = "default")
# 
# base::prop.table(table(train_down$default))
# base::table(train_down$default)
```

```{r}
# Model Formula

createModelFormula <- function(targetVar, xVars, includeIntercept = TRUE){
  if(includeIntercept){
    modelForm <- as.formula(paste(targetVar, "~", paste(xVars, collapse = '+ ')))
  } else {
    modelForm <- as.formula(paste(targetVar, "~", paste(xVars, collapse = '+ '), -1))
  }
  return(modelForm)
}

modelForm <- createModelFormula(targetVar, xVars)

```

```{r}
require(tree)
tree.default = tree(default~loan_amnt+term+int_rate+installment+grade+home_ownership+annual_inc+verification_status+purpose+dti+open_acc+pub_rec+total_acc+initial_list_status+Compensation.of.employees+Gross.operating.surplus+Per.capita.real.GDP.by.state+ Quantity.indexes.for.real.GDP.by.state+Real.GDP.by.state+Subsidies+Taxes.on.production.and.imports+regions, data=train)
summary(tree.default)

plot(tree.default)
text(tree.default, pretty = 0)

```



```{r}
#Base model for decision tree 

targetVar<- train$default
fit <- rpart(modelForm,
             data=train,
             method="class")

fit
summary(fit)

prp(fit)

## Predicting on test data
fit.pred = predict(fit, test, type="class")

## misclassification table
with(test, table(fit.pred, default))

## Confusion matrix 
test[,'fit.pred'] <- fit.pred
confusionMatrix(table(test$fit.pred, test$default))

```


```{r}
#cross validation to optimizes Hyperparameter tuning on decision tree

train_ctrl <- trainControl(method = "repeatedcv"
                       , number = 10, repeats = 3
                       , classProbs = TRUE
                       , summaryFunction = twoClassSummary  
                       , search = "grid"
                       , verboseIter = FALSE
                       , allowParallel = TRUE
                       )
x = train[,xVars]

y = train$default
#y = as.factor(train[, targetVar])
cleanNames <- function(x){
feature.names=names(x)
  for (f in feature.names) {
    if (class(x[[f]])=="factor") {
      levels <- unique(c(x[[f]]))
      x[[f]] <- factor(x[[f]],
                       labels=make.names(levels))
    }
  }
return(x)}

xOld <- x
x <- cleanNames(x)
y = make.names(y)

Cross_fit<- train(x = x
               , y = y
               , method = "rpart",
               tuneLength=20,
               metric = "ROC",
               trControl = train_ctrl)



Cross_fit

```

```{r}
# extract the model that caret developed

ggplot(Cross_fit)
fit2 <- Cross_fit$finalModel
prp(fit2)

test <- cleanNames(test)

fit2.pred <- predict(Cross_fit, test, type = "raw")

str(test)
str(fit2.pred)

levels(test$default) <- c("FALSE.","TRUE.")
test$default <- as.factor(test$default)
str(test$default)
ActualNew <-test$default

confusionMatrix(reference = test$default, data = fit2.pred)
varImp(Cross_fit)
```

```{r}
test1 <- read.csv("TestData.csv", header = TRUE)
test1[,'fit.pred'] <- fit2.pred
test1$fit.pred <- as.factor(test1$fit.pred)
levels(test1$fit.pred) <- c(FALSE, TRUE)

test1$fit.pred

```




```{r}
# saving test results in csv file

write.csv(test, file = "DecisionTreeTest.csv" )
```
















